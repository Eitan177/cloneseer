import pdb

import pandas as pd
import matplotlib.pyplot as plt
import sklearn
from Bio.Seq import Seq
from sklearn.svm import OneClassSVM
from scipy.signal import savgol_filter, find_peaks
from sklearn.cluster import DBSCAN
from Bio import pairwise2, motifs
import streamlit as st
import numpy as np
import re

def file_path_generation(sample_number):
    """
    This method generates out the file paths for all the files that are part of 1 sample

    :param sample_number: The sample number that needs to be entered to get all Frameworks and all Replicates
    :return: The file paths of all samples
    """

    files = []
    for framework_count in range(1, 4):
        for framework_value in ['A', 'B']:
            files.append(
                "Lymphotrack_Files/" + str(sample_number) + "FR" + str(framework_count) + framework_value + ".csv")
    return files


def create_mother_sequence_mask(mother_sequence):
    """
    Creates a trinucleotide mask for the mother sequence generated by CloneSeer.
    Shifts frame of 6 along the sequence and determines if  position 1 = position 4, position 2 = position 5,
    position 3 = position 6. If it matches, any 0 in the mask at that position turns into a 1

    :param mother_sequence: The most abundant sequence from data
    :return: Mask for mother sequence denoting trinucleotide
    """

    mother_sequence_trinucleotide = np.zeros(len(mother_sequence))
    for character_position in range(len(mother_sequence) - 6):
        if mother_sequence[character_position] == mother_sequence[character_position + 3] \
                and mother_sequence[character_position + 1] == mother_sequence[character_position + 4] \
                and mother_sequence[character_position + 2] == mother_sequence[character_position + 5]:
            for i in range(6):
                mother_sequence_trinucleotide[character_position + i] = 1
    return mother_sequence_trinucleotide


def trinucleotide(data, mother_sequence):
    """
    Creates columns that count the mismatches that occur in trinucleotides, deletions that occur in trinucleotides and
    insertions that occur in trinucleotides.
    :param data: Clone Seer file containing modified sequence, original sequence, number of times observed and inserts
    :param mother_sequence: The mother sequence that is observed most often
    :return: Columns representing mismatches that occur in trinucleotides, deletions that occur in trinucleotides and
    insertions that occur in trinucleotides.
    """
    mother_sequence_trinucleotide = create_mother_sequence_mask(mother_sequence)
    trinucleotide_mismatch_error = []
    dash_mismatch_error = []
    insert_mismatch_error = []
    for index, row in data.iterrows():
        trinucleotide_mismatch_count = 0
        dash_mismatch_count = 0
        insert_count = 0
        leadinga_gap=re.findall('^-+',row['sequence'])
        if leadinga_gap != []:
            gap_len=len(leadinga_gap[0])
        else:
            gap_len=0
        mother_sequence_no_leading_gap = mother_sequence[gap_len:]
        rowseq_no_leading_gap=row['sequence'][gap_len:]
        mother_sequence_trinucleotide_no_leading_gap = mother_sequence_trinucleotide[gap_len:]
        # Mismatches and deletions in trinucleotide checked using sequence from Clone Seer. Counts the number of
        # differences that appear between target sequence and mother sequence on a trinucleotide region

        for character_position in range(len(mother_sequence_no_leading_gap)):
            if rowseq_no_leading_gap[character_position] != mother_sequence_no_leading_gap[character_position] \
                    and rowseq_no_leading_gap[character_position] != "-" \
                    and mother_sequence_trinucleotide_no_leading_gap[character_position] == 1:
                trinucleotide_mismatch_count += 1
            if rowseq_no_leading_gap[character_position] != mother_sequence_no_leading_gap[character_position] and rowseq_no_leading_gap[character_position] == "-" and mother_sequence_trinucleotide_no_leading_gap[character_position] == 1:
                dash_mismatch_count += 1
        trinucleotide_mismatch_error.append(trinucleotide_mismatch_count)
        dash_mismatch_error.append(dash_mismatch_count)

        # Looks through the insert column and count the number of inserts that appear on the trinucleotide position.
        # Inserts at the end of the sequence do not increment insert count due to unreliability of sequencing at the end

        if row['inserts'] == '[]':
            insert_mismatch_error.append(0)
        else:
            insert_positions = row['inserts'][1:-1].split(',')
            for insert_position in insert_positions:
                if int(insert_position) > len(mother_sequence_trinucleotide) - 1:
                    insert_count += 0
                else:
                    if mother_sequence_trinucleotide[int(insert_position)] == 1:
                        insert_count += 1
            insert_mismatch_error.append(insert_count)
    return trinucleotide_mismatch_error, dash_mismatch_error, insert_mismatch_error


def get_total_deletions(data):
    """
    Counts the number of times a deletion occurs in a sequence represented by a dash in the sequence

    :param data: All data points
    :return: A column representing the number of deletions that appear in the sequence generated by Clone seer
    """
    return data.sequence.str.count('-')


def suspicious_mismatch(differences):
    """
    Initially creates a suspicious mismatch to find the max significant homology differences between
    rows immediately above and below. Final Homology difference scans for two consecutive numbers showing up indicating
    possible artifact. Converts point to min difference in order to remove artifact

    :param differences:
    :return: Suspicious mismatch: Greatest distance between the row above and below.
    """
    if len(differences) == 1:
        return [0]
    else:
        suspicious_mismatch_number = [abs(differences[0] - differences[1])]
        for differences_position in range(1, len(differences) - 1):
            suspicious_mismatch_number.append(max(abs(differences[differences_position] - differences[differences_position - 1]), abs(differences[differences_position] - differences[differences_position + 1])))
        suspicious_mismatch_number.append(abs(differences[len(differences) - 1] - differences[len(differences) - 2]))

        final_suspicious_mismatch_number = suspicious_mismatch_number.copy()

        for count in range(1, len(suspicious_mismatch_number) - 1):
            if suspicious_mismatch_number[count - 1] == suspicious_mismatch_number[count] and suspicious_mismatch_number[count] > 5:
                if differences[count - 1] > differences[count]:
                    final_suspicious_mismatch_number[count] = min(abs(differences[count] - differences[count - 1]), abs(differences[count] - differences[count + 1]))
            if suspicious_mismatch_number[count + 1] == suspicious_mismatch_number[count] and suspicious_mismatch_number[count] > 5:
                if differences[count + 1] > differences[count]:
                    final_suspicious_mismatch_number[count] = min(abs(differences[count] - differences[count - 1]), abs(differences[count] - differences[count + 1]))
        return final_suspicious_mismatch_number


def homology_metrics(data, row_of_max_observed, mother_sequence):
    """
    :param data: All data from clone seer
    :param row_of_max_observed: The row with the mother sequence
    :param mother_sequence: The mother sequence representing the sequence that is observed the most
    :return: differences: homology column denoting difference between target sequence and mother sequence
    :return: new_differences: Contiguous blocks of homology differences ex. Diff: [2,3,3,1,0,2,2] Result: [-3, -2, -1, 0, 1]
    """

    # Calculate differences regardless of trinucleotide between target sequence and mother clone
    differences = []
    for index, row in data.iterrows():
        leadinga_gap=re.findall('^-+',row['sequence'])
        if leadinga_gap != []:
            gap_len=len(leadinga_gap[0])
        else:
            gap_len=0
        mother_sequence_no_leading_gap = mother_sequence[gap_len:]
        rowseq_no_leading_gap=row['sequence'][gap_len:]
        differences.append(sum(a != b for a, b in zip(rowseq_no_leading_gap, mother_sequence_no_leading_gap)))

    # Calculate differences based upon differences and order. Mother sequence is 0.
    # Getting value from first half
    first_half_value = []
    current_difference = differences[row_of_max_observed]
    count = 0
    for i in range(row_of_max_observed, -1, -1):
        if differences[i] != current_difference:
            count -= 1
            first_half_value.append(count)
            current_difference = differences[i]
        else:
            first_half_value.append(count)
    # Getting Second Half Value
    second_half_value = []
    current_difference = differences[row_of_max_observed]
    count = 0
    for i in range(row_of_max_observed + 1, len(differences)):
        if differences[i] != current_difference:
            count += 1
            second_half_value.append(count)
            current_difference = differences[i]
        else:
            second_half_value.append(count)
    new_difference = first_half_value[::-1] + second_half_value
    suspicious_mismatch_number = suspicious_mismatch(differences)
    return differences, new_difference, suspicious_mismatch_number


def get_index_mismatch(data, mother_sequence):
    """
    Gets the indexes of mismatches between mother clone and the sequence being analyzed
    :param data: Clone seer file
    :param mother_sequence: The most abundant sequence
    :return: A list of list of indexes that are mismatches
    """
    index_mismatch = []
    sequences = data['sequence'].tolist()
    for sequence in sequences:
        index_mismatch.append([i for i in range(len(sequence)) if sequence[i] != mother_sequence[i]])
    return index_mismatch


def is_somatic_hypermutation(data, mother_sequence):
    """
    Gets all of the mismatch indexes between a data sequence and indexes and then shifts frames of 5 in order to find
    the difference between the 1st and 5th indexes to see if it's less than 20

    :param data: All data from clone seer
    :param mother_sequence: The most abundant sequence
    :return: An array of whether or not it is a somatic hypermutation. 0 is not a hypermutation. 1 is a hypermutation
    """

    somatic_hypermutation_result = []
    sequences = data['sequence'].tolist()
    for individual_sequence in sequences:
        mismatch_positions = []
        for sequence_character_position in range(len(individual_sequence)):
            if individual_sequence[sequence_character_position] != mother_sequence[sequence_character_position]:
                mismatch_positions.append(sequence_character_position)
        if len(mismatch_positions) < 5:
            somatic_hypermutation_result.append(0)
        else:
            somatic_hypermutation = 0  # False
            for mismatch_set_of_3_start_index in range(len(mismatch_positions) - 2):
                if mismatch_positions[mismatch_set_of_3_start_index + 2] - mismatch_positions[mismatch_set_of_3_start_index] <= 20:
                    somatic_hypermutation = 1  # True
            somatic_hypermutation_result.append(somatic_hypermutation)
    return somatic_hypermutation_result


def new_metrics(data):
    """
    Generates out more features for the data including deletion count, mismatch, deletion and insertion during trinucleotide,
    homology and contiguous homology differences with mother clone, as well as whether or not it is a somatic hypermutation
    :param data: All data from Clone Seer file
    :return: data: Data with new metrics columns attached
    """

    # Mother Clone Identification
    row_of_max_observed = np.argmax(data['alnscores'])
    mother_sequence = data['mother_clone'].iloc[0]
    # Number of deletions that appear in each sequence
    data['deletion_count'] = get_total_deletions(data)

    data['index_mismatch'] = get_index_mismatch(data, mother_sequence)

    # Mismatches, deletions, and insertions in trinucleotide regions
    data['mismatch_in_trinucleotide'], data['deletion_in_trinucleotide'], data['insertion_in_trinucleotide'] \
        = trinucleotide(data, mother_sequence)
    # Calculate homology differences and contiguous homology block differences
    data['homology_against_mother_clone'], data['contiguous_homology_against_mother_clone'], data['suspicious_mismatch'] = homology_metrics(data, row_of_max_observed, mother_sequence)

    data['somatic_mutations'] = is_somatic_hypermutation(data, mother_sequence)

    return data, row_of_max_observed, mother_sequence


def add_in_outlier_from_suspicious_mismatch(data, outliers_list, short_table):
    if short_table:
        suspicious_mismatch_results = data['suspicious_mismatch'].tolist()
        if np.min((len(suspicious_mismatch_results) + len(suspicious_mismatch_results) % 2 - 1)) != 1:
            smoothed_results = savgol_filter(suspicious_mismatch_results, window_length=np.min(
                (len(suspicious_mismatch_results) + len(suspicious_mismatch_results) % 2 - 1, 5)), polyorder=1)
        else:
            smoothed_results = savgol_filter(suspicious_mismatch_results, window_length=np.min(
                (len(suspicious_mismatch_results) + len(suspicious_mismatch_results) % 2 - 1, 5)), polyorder=0)
        peaks, properties = find_peaks(smoothed_results, prominence=1)
        new_outliers_list = outliers_list.copy()

        for individual_peak in peaks:
            search_area = suspicious_mismatch_results[individual_peak - 2: individual_peak + 2]
            if search_area != []:
                max_value = max(search_area)
                max_index = search_area.index(max_value)
                new_outliers_list[(individual_peak + max_index - 2)] = 2
        
        return new_outliers_list
    else:
        suspicious_mismatch_results = data['suspicious_mismatch'].tolist()

        smoothed_results = savgol_filter(suspicious_mismatch_results, window_length= 51, polyorder=1)
        peaks, properties = find_peaks(smoothed_results, prominence=2)
        new_outliers_list = outliers_list.copy()

        for individual_peak in peaks:
            search_area = suspicious_mismatch_results[individual_peak - 25: individual_peak + 25]
            max_value = max(search_area)
            max_index = search_area.index(max_value)
            new_outliers_list[(individual_peak + max_index - 25)] = 2
        return new_outliers_list

def add_in_outlier_from_homology_stats(data, outliers_list,short_table,threshold=5):
    if short_table:
        potential_new_outlier_index_right= np.where(np.array([0]+np.diff(data['homology_against_mother_clone']).tolist())>threshold)[0]
        potential_new_outlier_index_left= np.where(np.array(np.diff(data['homology_against_mother_clone']).tolist()+[0])>threshold)[0]
        potential_new_outlier_index = np.sort(np.concatenate((potential_new_outlier_index_right,potential_new_outlier_index_left)))
       
        if len(potential_new_outlier_index) >0:
            new_outliers_list=outliers_list.copy()
            for index in potential_new_outlier_index:
                
                if data['contiguous_homology_against_mother_clone'].iloc[index] not in  np.array(data['contiguous_homology_against_mother_clone'][np.where(outliers_list>0)[0]]):
                    new_outliers_list[index]=3
                    
            return new_outliers_list
        else:
            return outliers_list
    else:
        return outliers_list    

def get_high_num_observed_outliers(outlier_from_number_observed, row_of_mother_clone, short_table, ):
    """
    Returns an outlier list based upon if there is a high number of observed for the particular sequence.
    Built in order to consider the disparities of a low number of sequences needing a less restrictive look for high number
    """
    if short_table:
        if np.min((len(outlier_from_number_observed) + len(outlier_from_number_observed) % 2 - 1)) != 1:
            smoothed_results = savgol_filter(outlier_from_number_observed, window_length=np.min(
                (len(outlier_from_number_observed) + len(outlier_from_number_observed) % 2 - 1, 5)), polyorder=1)
        else:
            smoothed_results = savgol_filter(outlier_from_number_observed, window_length=np.min(
                (len(outlier_from_number_observed) + len(outlier_from_number_observed) % 2 - 1, 5)), polyorder=0)
        peaks, properties = find_peaks(smoothed_results, prominence=1)
        outliers_list = np.zeros(len(outlier_from_number_observed))
        outliers_list[row_of_mother_clone] = 1
        for individual_peak in peaks:
            search_area = outlier_from_number_observed[individual_peak - 2: individual_peak + 2]
            max_value = max(search_area)
            max_index = search_area.index(max_value)
            outliers_list[(individual_peak + max_index - 2)] = 1
            
        return outliers_list
    else:
        smoothed_results = savgol_filter(outlier_from_number_observed, window_length=51, polyorder=1)
        peaks, properties = find_peaks(smoothed_results, prominence=5)
        outliers_list = np.zeros(len(outlier_from_number_observed))
        outliers_list[row_of_mother_clone] = 1
        for individual_peak in peaks:
            search_area = outlier_from_number_observed[individual_peak - 25: individual_peak + 25]
            max_value = max(search_area)
            max_index = search_area.index(max_value)
            if max_value >= outlier_from_number_observed[row_of_mother_clone]/100:
                outliers_list[(individual_peak + max_index - 25)] = 1
        return outliers_list


def outlier_detection(file, short_table):
    """
    Takes in a file and finds all the outliers that can be subclones.
    1. Apply DBScan to all sequences except those who have a deletion or insertion on the trinucelotide and find the subclones
    2. Extract all deletions and find average and standard deviation. Deletions who have number observed greater than 1
    standard deviation about average are probable subclones
    3. Extract all insertions and find average and standard deviation. Insertions who have number observed greater than 1
    standard deviation about average are probable subclones
    :param file: File path of clone seer file
    :return: Array filled with likely subclones and mother clone in sequential order
    """

    data = file
    data_with_new_metrics, row_of_mother_clone, mother_clone = new_metrics(data)

    data_without_mother_clone = data_with_new_metrics.drop(index=row_of_mother_clone)
    data_with_insertion = data_without_mother_clone.loc[data_without_mother_clone['insertion_in_trinucleotide'] != 0]
    data_with_deletion = data_without_mother_clone.loc[data_without_mother_clone['deletion_in_trinucleotide'] != 0]
    outlier_from_number_observed = data_with_new_metrics['numberobserved'].tolist()
    outliers_list = get_high_num_observed_outliers(outlier_from_number_observed, row_of_mother_clone, short_table)

    df_insert_mean = data_with_insertion['numberobserved'].mean()
    df_insert_std = data_with_insertion['numberobserved'].std()
    df_deletion_mean = data_with_deletion['numberobserved'].mean()
    df_deletion_std = data_with_deletion['numberobserved'].std()
    df_insert_sequenceNotFormatted = data_with_insertion.loc[
        data_with_insertion['numberobserved'] >= df_insert_mean + df_insert_std]
    df_with_deletion_sequenceNotFormatted = data_with_deletion.loc[
        data_with_deletion['numberobserved'] >= df_deletion_mean + 0.5*df_deletion_std]

    for outlier_count in df_insert_sequenceNotFormatted['sequenceNotformatted'].index.tolist():
        outliers_list[outlier_count] = 1
    for outlier_count in df_with_deletion_sequenceNotFormatted['sequenceNotformatted'].index.tolist():
        outliers_list[outlier_count] = 1
    outliers_list = add_in_outlier_from_suspicious_mismatch(data_with_new_metrics, outliers_list, short_table)
    outliers_list = add_in_outlier_from_homology_stats(data_with_new_metrics,outliers_list,short_table)
    data['outliers'] = outliers_list
    outlier_sequential = []
    outlier_type = []
    for position in range(len(outliers_list)):
        if outliers_list[position] == 1:
            outlier_sequential.append(data['sequenceNotformatted'].iloc[position])
            outlier_type.append(1)
        if outliers_list[position] == 2:
            outlier_sequential.append(data['sequenceNotformatted'].iloc[position])
            outlier_type.append(2)
        if outliers_list[position] == 3:
            outlier_sequential.append(data['sequenceNotformatted'].iloc[position])
            outlier_type.append(3)            
    return data, outlier_sequential, outlier_type


def partition(data, outliers, outlier_type):
    """
        Takes the data and for each sequence assigns a set of alignment score (one against each outlier) and then picks the one
        that has the least difference between the sequence length and the target sequence.
        :param data: Clone Seer Data
        :param outliers: All of the outliers
        :param outlier_type: The outlier type. 1 = Outlier from trinucleotide or num observed. 2 = somatic hypermutation
        :return: Data with a column for closest match
    """
    sequences = data['sequenceNotformatted'].tolist()
    print(len(outliers))
    # First Pass utilizing only outliers coming from num observed filter
    initial_outlier_class = [i for i, x in enumerate(outlier_type) if x == 1]  # Gets all of the indexes of outliers that came from the original num observed filter
    initial_outlier_class_corresponding_sequence = [] # Corresponding sequence to num observed filter outliers
    for individual_outlier_class in initial_outlier_class:
        initial_outlier_class_corresponding_sequence.append(outliers[individual_outlier_class])
    initial_outlier_index = []  # Corresponding Outlier Index Value in the actual outliers
    for individual_outlier in initial_outlier_class_corresponding_sequence:
        initial_outlier_index.append(data.index[data['sequenceNotformatted'] == individual_outlier])

    # First pass matching
    closest_match = []
    count = 0
    for sequence in sequences:
        outlier_score = len(sequence)
        best_outlier_position = -1
        outlier_seq_pos = 2 * len(sequences)
        sequence_no_dash = sequence.replace("-", "")

        for outlier_count in range(len(initial_outlier_class_corresponding_sequence)):
            alignments = pairwise2.align.globalms(sequence_no_dash, initial_outlier_class_corresponding_sequence[outlier_count].replace("-", ""), 1, -1, -1, 0, score_only=True)
            if len(sequence_no_dash) - alignments < outlier_score:
                outlier_score = len(sequence_no_dash) - alignments
                best_outlier_position = initial_outlier_class[outlier_count]
                outlier_seq_pos = initial_outlier_index[outlier_count]
            if len(sequence_no_dash) - alignments == outlier_score:
                if outlier_seq_pos - count > initial_outlier_class[outlier_count] - count:
                    best_outlier_position = initial_outlier_class[outlier_count]
                    outlier_seq_pos = initial_outlier_class[outlier_count]
        closest_match.append(best_outlier_position)
        count += 1

    # Grabbing original index for second pass. Takes 20 above and 20 below for each outlier and sees if class needs to be adjusted with better score
    outlier_index = []  # Corresponding Outlier Index Value in the actual outliers
    for individual_outlier in outliers:
        outlier_index.append(data.loc[data['sequenceNotformatted'] == individual_outlier].index.values)
    outlier_class = 0
    for individual_outlier_index in outlier_index:
        for search_area in range(max(0, individual_outlier_index[0] - 20), min(individual_outlier_index[0] + 21, len(sequences))):
            new_score = pairwise2.align.globalms(sequences[search_area].replace("-", ""), sequences[individual_outlier_index[0]].replace("-", ""), 1, -1, -1, 0, score_only=True)
            old_score = pairwise2.align.globalms(sequences[search_area].replace("-", ""), outliers[closest_match[search_area]].replace("-", ""), 1, -1, -1, 0, score_only=True)
            if new_score > old_score:
                closest_match[search_area] = outlier_class
        outlier_class += 1
    data['closest match'] = closest_match
    
    return data


def partitionV2(data, outliers, outlier_type, variantoutliers):
    """
        Takes the data and for each sequence assigns a set of alignment score (one against each outlier) and then picks the one
        that has the least difference between the sequence length and the target sequence.
        :param data: Clone Seer Data
        :param outliers: All of the outliers
        :param outlier_type: The outlier type. 1 = Outlier from trinucleotide or num observed. 2 = somatic hypermutation
        :param variantoutliers: All the outliers found by trinucleotide but trimmed to variants
        :return: Data with a column for closest match
    """
    sequences_pass1 = data['variantseq'].tolist()
    sequences = data['sequenceNotformatted'].tolist()
    # First Pass utilizing only outliers coming from num observed filter
    initial_outlier_class = [i for i, x in enumerate(outlier_type) if x == 1]  # Gets all of the indexes of outliers that came from the original num observed filter
    initial_outlier_class_corresponding_sequence = []  # Corresponding sequence to num observed filter outliers
    for individual_outlier_class in initial_outlier_class:
        initial_outlier_class_corresponding_sequence.append(outliers[individual_outlier_class])
    initial_outlier_index = []  # Corresponding Outlier Index Value in the actual outliers
    for individual_outlier in initial_outlier_class_corresponding_sequence:
        initial_outlier_index.append(data.index[data['sequenceNotformatted'] == individual_outlier])

    # First pass matching
    closest_match = []
    count = 0
    for sequence in sequences_pass1:
        outlier_score = len(sequence)
        best_outlier_position = -1
        outlier_seq_pos = 2 * len(sequences_pass1)
        sequence_no_dash = sequence.replace("-", "-")
        for outlier_count in range(len(variantoutliers)):
            alignments = pairwise2.align.globalms(sequence_no_dash, variantoutliers[outlier_count].replace("-", "-"), 1, -1, -1, 0, score_only=True)
            if alignments != []:
                if len(sequence_no_dash) - alignments < outlier_score:
                    outlier_score = len(sequence_no_dash) - alignments
                    best_outlier_position = initial_outlier_class[outlier_count]
                    outlier_seq_pos = initial_outlier_index[outlier_count]
                if len(sequence_no_dash) - alignments == outlier_score:
                    if outlier_seq_pos - count > initial_outlier_class[outlier_count] - count:
                        best_outlier_position = initial_outlier_class[outlier_count]
                        outlier_seq_pos = initial_outlier_class[outlier_count]
        closest_match.append(best_outlier_position)
        
        count += 1

    # Grabbing original index for second pass. Takes 20 above and 20 below for each outlier and sees if class needs to be adjusted with better score
    outlier_index = []  # Corresponding Outlier Index Value in the actual outliers
    for individual_outlier in outliers:
        outlier_index.append(data.loc[data['sequenceNotformatted'] == individual_outlier].index.values)
    outlier_class = 0
    for individual_outlier_index in outlier_index:
        for search_area in range(max(0, individual_outlier_index[0] - 20),
                                 min(individual_outlier_index[0] + 21, len(sequences))):
            new_score = pairwise2.align.globalms(sequences[search_area].replace("-", ""),
                                                 sequences[individual_outlier_index[0]].replace("-", ""), 1, -1, -1, 0,
                                                 score_only=True)
            old_score = pairwise2.align.globalms(sequences[search_area].replace("-", ""),
                                                 outliers[closest_match[search_area]].replace("-", ""), 1, -1, -1, 0,
                                                 score_only=True)
            if new_score > old_score:
                closest_match[search_area] = outlier_class
        outlier_class += 1
    data['closest match'] = closest_match
    return data



def process_fasta_from_ebi(fasta_file_path, data,id_separator="|",id_separator_index=1,element_to_match="v-genes"):
    """
    The function takes the fasta_file generated from EBI listing the V genes and matches it up against the outliers
    :param fasta_file_path: The path to the fasta file containing all of the genes
    :param data: Data with all of the metrics
    :return: Data that has a new column denoting V-gene
    """

    # Builds the IGHV gene database from the EBI file
    fasta_file = open(fasta_file_path, "r")
    igh_element_database = {}
    current_key = ""
    current_sequence = ""
    for line in fasta_file:
        if line[0] == '>':
            split_title_line = line.split(id_separator)
            igh_element_database[current_key] = current_sequence.replace("\n", "").upper()
            current_key = split_title_line[id_separator_index]
            current_sequence = ""
        else:
            current_sequence = current_sequence + line
    del igh_element_database[""] # Needs to delete first item in order to place

    corresponding_igh_element = []
    corresponding_score = []
    corresponding_percent = []
    corresponding_fullmatch_query=[]
    corresponding_fullmatch_ig_element=[]
    corresponding_endmatch=[]
    
    data_sequences = data['sequence']
    data_outlier = data['outliers']

    for individual_sequence_position in range(len(data_sequences)):
        if data_outlier[individual_sequence_position] == 1 or data_outlier[individual_sequence_position] == 2:
            max_score = 0
            max_percent=0
            endmatch=0
            max_score_key = ""
            indiv_seq = data_sequences[individual_sequence_position]
            lenindiv=len(indiv_seq.replace('-',''))
            fullmatch_list1=[]
            fullmatch_list2=[]
            print(individual_sequence_position )
            a=0
            for key in igh_element_database.keys():

                
                
                '''    custom_matching={('A','A'):1,('G','G'):1,('C','C'):1,('T','T'):1,
                                    ('N','A'):1,('N','T'):1,('N','C'):1,('N','G'):1,
                                    ('A','T'):0,('A','C'):0,('A','G'):0,
                                    ('T','A'):0, ('T','C'):0, ('T','G'):0,
                                    ('G','C'):0,('G','A'):0,('G','T'):0,
                                    ('C','G'):0,('C','A'):0,('C','T'):0,
                                    ('.','A'):1,('.','T'):1,('.','C'):1,('.','G'):1}  
                '''
                    
                alignment_score = pairwise2.align.localms(indiv_seq, igh_element_database[key][-(lenindiv):].replace('.',''), 1,-1, -1, -.2, score_only=True)

                #if key=='IGHV4-59*04' or key=='IGHV3-21*01':
                #if indiv_seq=='GCCTCTGGATTCACCTTCAGTAGCTATAGCATGAACTGGGTCCGTCAGGCTCCAGGGAAGGAGCTGGAGTGGGTCTCATCCATTAGTAGTAGTAGTAGTTACATATACTACGCAGACTCAGTGAAGGGCCGATTCACCATCTCCAGAGACAACGCCAAGAACTCACTGTATCTGCAAATGAACAGCCTGAGAGCCGAGGACACGGCTGTGTATTACTGTGCGAGTCAG----------GGTGGTAACTCCGTTTC----TGCTTTTGATATCTGGGGCCAAGGGACAAT' and (key=='IGHV3/OR16-9*01' or key=='IGHV3-21*01'):
                #    breakpoint()  
                #if float(alignment_score) / float(len(igh_element_database[key])) > max_score:
                if (float(alignment_score)  >= max_score and float(alignment_score)>0 and float(alignment_score) / float(len(igh_element_database[key][-(lenindiv):].replace('.',''))) > max_percent) or endmatch<5: 

                    alignment_full=pairwise2.align.localms(indiv_seq, igh_element_database[key][-(lenindiv):].replace('.',''),1,-1, -1, -.2)[0]
                    #if key=='IGHV7-34-1*01':
                    #    breakpoint()
                    list1=[]
                    list2=[]
                    if re.findall('V',element_to_match.upper()) != []:
                        gap=re.findall('^-+',alignment_full.seqB[::-1])
                        if gap != []:
                            gap_len=len(re.findall('^-+',alignment_full.seqB[::-1])[0])
                        else:
                            gap_len=0
                        list1[:]=alignment_full.seqB[::-1][gap_len:]
                        list2[:]=alignment_full.seqA[::-1][gap_len:]
                        prefullmatch_list1=re.sub('-+$','',re.sub('^-+','',''.join(list1)))[::-1]
                        prefullmatch_list2=re.sub('-+$','',re.sub('^-+','',''.join(list2)))[::-1]
                        #if lenindiv<100:
                        #if a==1:
                        #    breakpoint()
                        pre_endmatch=np.where(pd.DataFrame({'a':list1,'b':list2}).apply(lambda x: x[0]==x[1],axis=1)==False)[0]
                        if len(pre_endmatch)>0:
                            if pre_endmatch[0]<=15 and len(pre_endmatch)>1 and max(pre_endmatch)>15:
                                endmatch=pre_endmatch[1:][np.where(pre_endmatch[:-1:]- pre_endmatch[1:] <= -5)[0][0]] 
                                        
                            else:
                                endmatch=pre_endmatch[0]

                        else:
                            endmatch=len(list1)
                    else:
                        gap=re.findall('^-+',alignment_full.seqB)
                        if gap != []:
                            gap_len=len(re.findall('^-+',alignment_full.seqB)[0])
                        else:
                            gap_len=0  
                       
                        list1[:]=alignment_full.seqB[gap_len:]
                        list2[:]=alignment_full.seqA[gap_len:] 
                        prefullmatch_list1=re.sub('-+$','',re.sub('^-+','',''.join(list1)))
                        prefullmatch_list2=re.sub('-+$','',re.sub('^-+','',''.join(list2)))      
                        endmatch=np.where(pd.DataFrame({'a':list1,'b':list2}).apply(lambda x: x[0]==x[1],axis=1)==False)[0][0]    
                    
                    if endmatch>=5:                    #max_score = float(alignment_score) / float(len(igh_element_database[key]))
                        max_score = float(alignment_score)
                        max_percent = float(alignment_score) / float(len(igh_element_database[key][-(lenindiv):].replace('.','')))
                        max_score_key = key
                        fullmatch_list1 = prefullmatch_list1
                        fullmatch_list2 = prefullmatch_list2

            corresponding_igh_element.append(max_score_key)
            corresponding_score.append(max_score)
            corresponding_percent.append(max_percent)
            corresponding_fullmatch_query.append(fullmatch_list2)
            corresponding_fullmatch_ig_element.append(fullmatch_list1)
            corresponding_endmatch.append(endmatch)
            
        else:
            corresponding_igh_element.append('Not Outlier')
            corresponding_score.append(0)
            corresponding_percent.append(0)
            corresponding_fullmatch_query.append(0)
            corresponding_fullmatch_ig_element.append(0)
            corresponding_endmatch.append(0)

    
    data[element_to_match] = corresponding_igh_element
    data[element_to_match+ ' score'] = corresponding_score
    data[element_to_match+ ' percent'] = corresponding_percent
    data[element_to_match+' endmatch'] = corresponding_endmatch
    data[element_to_match+' fullmatch query'] = corresponding_fullmatch_query
    data[element_to_match+' fullmatch ig element'] = corresponding_fullmatch_ig_element
    
    return data


def KevinsFunction(table_of_sequences):
    table_of_sequences['inserts'] = table_of_sequences['inserts'].apply(lambda x: str(x))
    short_table = True
    if len(table_of_sequences) > 1000:
        short_table = False
       
    data, found_outlier, outlier_type = outlier_detection(table_of_sequences, short_table)
    
    # 07/12/2023 remove
    #v_gene_data = process_fasta_from_ebi('IGHV.fasta', data)
    

    #partitioned_data = partitionV2(v_gene_data, found_outlier, outlier_type, outlierseqmutated)
    partitioned_data = partition(data, found_outlier, outlier_type)
    return partitioned_data
